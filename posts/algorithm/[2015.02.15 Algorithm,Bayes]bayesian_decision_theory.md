# Bayesian Decision Theory 学习笔记

## INTRODUCTION

This lack of knowledge is indicated by modeling the process as a random process.

由于知识匮乏，我们将一些过程定义为随机过程。但是如果我们掌握了足够的知识，那么一切都是可推导的了。

### Unobservable Variables

The extra pieces of knowledge that we do not have access to are named the unobservable variables.

`不可观测变量`：我们无法获取的知识称为不可观测变量，标记为 $z$。

Denoting the unobservables by $z$ and the observable as $x$, in reality we have

$$
x = f(z)
$$

where $f(z)$ is the deterministic function that defines the outcome from the unobservable pieces of knowledge.

If we do not know $P(X)$ and want to estimate this from a given sample, then we are in the realm of statistics. We have a sample, $X$, containing examples drawn from the probability distribution of the observables $x$, denoted as $P(x)$. The aim is to build an approximator to it, $\hat{P}(x)$, using the sample $X$.

我们无法得知 $P(X)$ 的概率模型，但是我们可以通过统计大量的实例，尽力构建一个相近的模型。

## CLASSIFICATION

### Prior Probability

It is called the prior probability because it is the knowledge we have as to the value of $C$ before looking at the observables $x$, satisfying

$$
P (C = 0) + P (C = 1) = 1
$$

`先验概率`：在观察 $x$ 前，我们已经知道 $C$ 取某值时的概率。

### Class Likelihood

$P(x|C)$ is called the class likelihood and is the conditional probability that an event belonging to $C$ has the associated observation value $x$.It is what the data tells us regarding the class.


`类似然`：属于 $C$ 的事件具有相关联的观测值 $x$ 的条件概率。这就是数据告诉我们的关于类的信息。

### Evidence

$P(x)$, the evidence, is the marginal probability that an observation $x$ is seen, regardless of whether it is a positive or negative example.


`证据`：事件 $x$ 发生的概率。

### Posterior Probability

Combining the prior and what the data tells us using Bayes' rule, we calculate the posterior probability of the concept, $P(C|x)$, after having seen the observation, $x$.

$$
posterior = \frac{prior * likelihood}{evidence}
$$

`后验概率`：使用贝叶斯规则，组合先验知识和数据，使用公式计算后验概率。

Because of normalization by the evidence, the posteriors sum up to $1$:

$$
P(C = 0|x) + P(C = 1|x) = 1
$$

### Bayes' Classifier

the Bayes' classifier chooses the class with the highest posterior probability; that is, we

$$
\text{choose}\ C_{i}\ \text{if}\ P(C_{i}|x)=\underset{k}{\text{max}}\ P(C_{k}|x)
$$

`贝叶斯分类器`：选择后验概率最大的类别作为分类结果。

## LOSSES AND RISKS

A finan- cial institution when making a decision for a loan applicant should take into account the potential gain and loss as well.The situation is much more critical and far from symmetry in other domains like medical diagnosis or earthquake prediction.


`不对等的决策收益和风险`：在不同的领域，做出接受或拒绝的决策造成的收益和风险不是对等的，比如在医疗预测和地震预测等领域。所以我们应该将领域的不同纳入决策考虑范围。

### Loss Function & Expected Risk

Let us define action αi as the decision to assign the input to class $C_i$ and $\lambda_{ik}$ as the loss incurred for taking action ${\alpha}_i$ when the input actually belongs to $C_k$. Then the expected risk for taking action ${\alpha}_i$ is:

$$
R(\alpha_{i}|x)=\sum_{k=1}^{K}\lambda _{ik}P(C_{k}|x)
$$

and we choose the action with minimum risk:

$$
\text{choose }\alpha _{i}\text{ if }R(\alpha_{i}|x)=\underset{k}{\text{min}}\ R(\alpha_{k}|x)
$$

`风险期望`：划分到别的类的概率乘划分到该类的损失。


`决策方式`：选择使风险期望最小的决策。

Thus to minimize risk, we choose the most probable case.

### Reject

In such a case, we define an additional action of reject or doubt, ${\alpha}_{K+1}$ with ${\alpha}_i, i = 1, \cdots, K$, being the usual actions of deciding on classes $C_i, i = 1, \cdots, K$.


`拒绝决策`：当决策风险很高时，分类系统需要额外定义一个拒绝分类的类别。我们同样通过计算风险期望来做出最优决策。

A possible loss function is:

$$
\lambda _{ik} = \begin{cases} 0 & \text{ if } i= k\\ \lambda & \text{ if } i=K+1 \\ 1 & \text{ otherwise } \end{cases}
$$

where $0 \lt \lambda \lt 1$ is the loss incurred for choosing the (K + 1)st action of reject. Then the risk of reject is

$$
R(\alpha_{k+1}|x) = \sum_{k=1}^{K}\lambda P(C_{k}|x) = \lambda
$$

## UTILITY THEORY

We now generalize this to utility theory, which is concerned with making rational decisions when we are uncertain about the state. Let us say that given evidence $x$, the probability of state $S_k$ is calculated as $P(S_k|x)$. We define a utility function, $U_{ik}$, which measures how good it is to take action ${\alpha}_i$ when the state is $S_k$. The expected utility is:

$$
EU(\alpha _{i} | x)=\sum_{k}U_{ik}P(S_{k}|x)
$$

A rational decision maker chooses the action that maximizes the expected utility.

`实用理论`：最小风险 = 最大收益，选择能获得最大预期收益的决策。

if we know how much money we will gain as a result of a correct decision, how much money we will lose on a wrong decision, and how costly it is to defer the decision to a human expert, depending on the particular application we have, we can fill in the correct values $U_{ik}$ in a currency unit, instead of $0$, $\lambda$, and $1$, and make our decision so as to maximize expected earnings.

量化决策成本，使决策有理可循。

正确分类的收益，错误决策的损失，移交专家的成本，我们通过量化所有的收益和成本，就可以做出最精确决策而不需要纠结。这也是做人的道理。

## ASSOCIATION RULES

### Association Rule

An association rule is an implication of the form $X \rightarrow Y$ where X is the antecedent and $Y$ is the consequent of the rule.


`关联规则`：先行条件 $X$ 到后续结果 $Y$ 的关系。

In learning association rules, there are three measures that are frequently calculated:

* Support of the association rule $X \rightarrow Y$:

$$
\text{Support}(X, Y)\equiv P(X Y)
$$

* Confidence of the association rule $X \rightarrow Y$ :

$$
\text{Confidence}(X\rightarrow Y) \equiv P(Y|X)
$$

* Lift, also known as interest of the association rule $X \rightarrow Y$ :

$$
\text{Lift}(X \rightarrow Y) \equiv \frac{P(Y|X)}{P(Y)}
$$

Support shows the statistical significance of the rule, whereas confidence shows the strength of the rule.

支持反映了规则的统计意义，而自信度体现了规则的强度。

### Apri-ori Algorithm

There is an efficient algorithm, called Apri-ori (Agrawal et al. 1996) that does this, which has two steps.


`演绎算法`：快速找出拥有足够支持度和自信度的协议规则。

1. That is, we only need to check for three-item sets all of whose two-item subsets are frequent; or, in other words, if a two-item set is known not to be frequent, all its supersets can be pruned and need not be checked.


`快速找到高频率的物品组`：如果 $\{X,Y,Z\}$ 频繁出现，那么 $\{X,Y\}$，$\{Y,Z\}$，$\{X,Z\}$ 必然也频繁出现。基于这个简单地推理，我们可以快速地删除不符合条件的物品组。

2. We start by finding the frequent one-item sets and at each step, inductively, from frequent $k$-item sets, we generate candidate $k+1$-item sets and then do a pass over the data to check if they have enough support. The Apriori algorithm stores the frequent itemsets in a hash table for easy access. Note that the number of candidate itemsets will decrease very rapidly as $k$ increases.


我们通过 $k$ 物品组推导出 $k+1$ 物品组。我们使用哈希表存储频繁物品组以加速访问。随着 $k$ 的增加候选的物品组将快速减小。

### Hidden Variables

It should be kept in mind that a rule $X \rightarrow Y$ need not imply causality but just an association. In a problem, there may also be hidden variables whose values are never known through evidence. The advantage of using hidden variables is that the dependency structure can be more easily defined. 


`隐藏变量`：$X \rightarrow Y$ 的关联规则不一定说明 $X$ 到 $Y$ 是因果关系，仅仅是一种联系。隐藏变量 $Z$ 可能导致 $X$ 和 $Y$ 有大概率发生，产生的表象就是 $X$ 与 $Y$ 有很强的联系。
