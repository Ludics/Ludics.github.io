# 朴素贝叶斯分类的 Python 实现和思考

最近在忙毕业设计，涉及到机器学习的一些经典的分类方法。最近就仔细的研究了一下朴素贝叶斯分类，并使用 Python 实现了一个简单的分类器，在一些数据库上也得到不错的效果，遂写篇文章总结一下。

## 关于朴素贝叶斯分类

关于朴素贝叶斯分类，之前写的一篇笔记介绍过它的原理了，在这里：[Bayesian Decision Theory 学习笔记](/algorithm/bayesian_decision_theory.html)。

更详细的可以参考维基百科上的词条：[朴素贝叶斯分类器](http://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8)，[Naive Bayes classifier](http://en.wikipedia.org/wiki/Naive_Bayes#Sex_classification)。

本着不重复造轮子的原则，原理我就不写了，维基中文英文都写得很详细了…

## 基于朴素贝叶斯的文本分类

基于朴素贝叶斯的文本分类，上面维基百科里也提到了。这里再贴一篇论文：[改善朴素贝叶斯在文本分类中的稳定性](http://www.cnki.net/KCMS/detail/detail.aspx?QueryID=2&CurRec=1&recid=&filename=ZGZR200411001019&dbname=CPFD9908&dbcode=CPFD&pr=&urlid=&yx=&uid=WEEvREcwSlJHSldTTGJhYkdIcjVnUU9xL2MzQmh6c2MrUm10YUt2TFZCdHQrRlNPK0pFMU1lUDZmY3hyYk85STkrTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&v=MDAwNDFNT1VLcmlmWnVOdkZTdmpVN2JOSWx3Y1B5clJmTEc0SHRYTnJvOUZaZXNPQlJOS3VoZGhuajk4VG5qcXF4ZEVl)。

本着不重复造轮子的原则，我简单说一下我自己的理解。

### 二项独立模型

二项独立模型（Binary Independence Model，BIM），侧重于考察文档 $d$ 里有没有出现单词 $w$。假设 $c_i$ 分类的任一文档里出现单词 $w$ 的概率是 $p(w | c_i)$，那么对于含单词 $w$ 的文档 $d$，仅从单词 $w$ 考察，它属于 $c_i$ 分类的概率为：

$$
P(C_{i} | d) = \frac {P(C_{i})P(w | C_{i})} {P(d)}
$$

同时，$c_i$ 分类的任一文档里不出现单词 $w$ 的概率为 $1 - p(w | c_i)$ 。对于不含单词 $w$ 的文档 $d$，仅从单词 $w$ 考察，它属于 $c$ 分类的概率为：

$$
P(C_{i} | d) = \frac {P(C_{i})(1-P(w | C_{i}))} {P(d)}
$$

### 朴素二字

朴素（naive）的含义，我们假设单词 $w_1$ 和单词 $w_2$ 在任一文档中是否出现是两个独立的事件。当然这是错误的，但是经验告诉我们基于这种假设的朴素贝叶斯分类的效果很好。基于这 naive 的假设，我们把文档 $d$ 内所有的单词（$w_k, 1 \le k \le n$）都加入到分类的考量范围里：

$$
P(C_{i} | d) = \frac {P(C_{i})\prod_{k=1}^{n}P(w_{k} | C_{i})} {P(d)}
$$

### 概率估计

由于我们无法统计这个世界上所有的文档，我们只能通过样本数据统计 $p(c_i)$ 和 $p(w_k | c_i)$ 的概率。

对于 $p(c_i)$，很多时候我们也无法统计出来。大多数情况下我们的样本并不是随机的从这个世界里抽取出来的，比如一个包含大量垃圾邮件的数据集中可能近一半都是垃圾邮件，垃圾邮件是人们特意提取出来用作训练分类器的，现实世界中可能与这个概率相差甚大(可能90%都是垃圾邮件也说不定…)。所以我们如果使用样本中类别为 $c_i$ 的概率估计现实世界中 $c_i$ 的概率是有偏的。但是经验告诉我们基于这个的假设训练出来的分类器效果还是很好..。通常直接使用类别为 $c_i$ 的文档数除以总的文档数。

对于 $p(w_k | c_i)$，我们考量的是类别 $c_i$ 中任一一篇文档里出现单词 $w_k$ 的概率，所以理论上我们应该使用下面的公式估计 $p(w_k | c_i)$ 的值：

$$
P(w_{k} | c_{i}) = \frac {n_{ik}} {n_{i}}
$$

$n_{ik}$ 表示属于 $c_i$ 类别的样本中含有单词 $w_k$ 的文档数，$n_i$ 表示属于 $c_i$ 类别的样本总数。但是，我们的样本只是现实世界中的一小部分。如果我们收集到的类别为 $c_i$ 的样本中没有文档里含有 'love' 这个单词，当我们判别一篇含有 'love' 的文档是不是属于 $c_i$ 时，估计的 $p(\text {love} | c_i)$ 的概率即为 $0$，使得计算的 $p(c_i | d)$ 的概率也为 $0$。发生这一切就是因为我们没有收集到足够的文档。显然这是不对的，而现实又不可能让我们收集到足够的文档，我们需要对估计 $p(w_k | c_i)$ 进行一些调整。

### 平滑算法

首先贴个维基百科链：[Additive Smoothing](http://en.wikipedia.org/wiki/Additive_smoothing)，[Good–Turing frequency estimation](http://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation)，[图灵估计](http://zh.wikipedia.org/wiki/%E5%9B%BE%E7%81%B5%E4%BC%B0%E8%AE%A1)。

平滑算法是用来处理由于收集的数据不足而导致的某些数据项估计频率为 $0$ 的问题。以最简单的 $Laplace$ 平滑为例（中文翻译是拉普拉斯平滑…），我们使用下面的公式估计 $p(w_k | c_i)$ 的值：

$$
P(w_{k} | c_{i}) = \frac {n_{ik} + 1} {n_{i} + 2}
$$

我们给出现单词 $w_k$ 的属于 $c_i$ 类别的文档数 $+1$，给属于 $c_i$ 类别的所有文档数 $+2$。这样我们就保证了 $p(w_k | c_i)$ 的概率始终大于 $0$。

这样 $+1$，$+2$ 看起来很随意啊，怎么理解呢? 我自己的理解是，我们在类别 $c_i$ 中加入了一篇文档，这篇文档里含有$w_1, w_2, \cdots, w_n$。所以出现 $w_k$ 的文档数都加上了 $1$。

那么为什么 $n_i$ 加了 $2$? 不是 $+1$ 就保证大于 $0$ 了吗? 因为我们同时要保证 $1 - p(w_k | c_i)$ 不为 $0$，所以 $n_i$ 一定要比 $n_{ik}$ 大。可以理解为我们在 $c_i$ 类别里加入了一篇文档，这篇文档里不含有 $w_1, w_2, \cdots, w_n$ 任一一个单词。

> 上述内容包含大量的个人推断，不保证一定正确。有不同想法欢迎交流^\_^。

从上面的公式中，我们可以发现这简单的 $+1$，$+2$ 还是有它自己的意义的，不是随意被人们定下来的，稍微想想也挺有意思。上方贴出的论文里也提到了一些。

另外我们也可以看到，Laplace 平滑还是过于简单粗糙了。所以伟大的前辈们又提出了很多更好的平滑算法，比如图灵估计（Good-Turing），Katz 改进平滑算法，Kneser-Ney 平滑算法等等。可以参考这里的一篇博文：[关于数据平滑的一些理解](http://blog.sina.com.cn/s/blog_6962921f0100q8bd.html)。

### 多项式模型

多项式模型（Multinomial Model，MM），不同于 BIM 仅仅统计一个词是否在文档中出现，而是统计类别中出现单词 $w$ 的概率，这样单词出现的频数也会纳入考量。

假设 $c_i$ 分类的所有文档里，单词 $w$ 出现 $1$ 次的概率是 $p(w | c_i)$，并且单词 $w$ 是否多次出现符合二项分布且相互独立，那么对于含 $f$ 个单词 $w$ 的文档 $d$，仅从单词 $w$ 考察，它属于 $c_i$ 分类的概率为：

$$
P(C_{i} | d) = \frac {P(C_{i})P(w | C_{i})^{f}} {P(d)}
$$

特别的，如果文档中没有出现单词 $w$，而统计的单词表中有单词 $w$，那么仅从单词 $w$ 考察，该文档属于 $c_i$ 分类的概率为：

$$
P(C_{i} | d) = \frac {P(C_{i})(1-\sum_{f=1}^{\infty}P(w|C_{i})^{f})}{P(d)}
$$

通过计算等比数列的和，我们可以将上式简化为：

$$
P(C_{i} | d) = \frac {P(C_{i})} {P(d)(1-P(w | C_{i}))}
$$

同样的，多项式模型对p(w_k | c_i)的估计如下：

$$
P(w_{k} | c_{i}) = \frac {N_{ik}} {\sum_ {k=1}^{|V|}N_{ik}}
$$

Nik表示单词 $w_k$ 在分类 $c_i$ 的样本中出现的次数，分母也就是分类 $c_i$ 的样本中统计的单词总数。$|V|$ 代表统计的单词表的大小，即多少种单词。

使用 Laplace 修正后的估计如下：

$$
P(w_{k} | c_{i}) = \frac {N_{ik} + 1} {\sum_ {k=1}^{|V|}N_{ik} + |V|}
$$

理解为每个单词的频数都加上 $1$，分母自然加上了单词的种数。

### 特征降维

实际使用朴素贝叶斯进行文本分类时，文本内的单词总量可能很大，严重影响了计算的速度。我们需要选择一些有代表的单词降低计算的复杂度。而这里选择哪些单词也是一个问题。

我们可以使用停用词表，删除一些非常常用的无实际意义的词，如：“是”，“吗”，“嗯”，等。另外我们可以选择一些在不同类别中出现概率相差很大的词，以提高分类的区分度。关于这一块的内容，笔者也会在近期学习并实验。本次实验将直接选择所有出现的词，以及出现频次排名较高的单词。

### 计算处理

实际计算过程中，由于计算的是概率的连乘，当单词表很大时，结果会接近于 $0$，而导致无法比较。目前一般使用的方法是使用 log 运算将乘法转化为加法，除法转化为减法。这种方法在 ACM 竞赛里也有不少应用，也算是数学之美了。

## 使用 NLTK 的朴素贝叶斯分类器

NLTK 是一个 Python 的自然语言处理库，内部实现了一个非常实用的朴素贝叶斯分类器。可以直接使用 pip 安装。

对于测试数据集，我们可以在网上找到很多提供数据集的网站。比如这里：[http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)，这是《Machine Learning in Action》书上推荐的数据集站点。我找到了一个古老的 Spambase 的数据集，直接提供的向量模型的数据，下载页面在这里：[http://archive.ics.uci.edu/ml/datasets/Spambase](http://archive.ics.uci.edu/ml/datasets/Spambase)。

Spambase 数据集下载解压后，文件夹里的 spambase.data 文件里就是所有的数据了，每行 59 列，使用逗号隔开，最后一列为 1 则代表是垃圾邮件。数据集里一共有 4601 封邮件，其中 1813 封垃圾邮件。详细的每列数据的含义看以参看同文件夹下的 spambase.names 文件。

对于该数据集，我们除去倒数第二列到倒数第四列的数据，关于单词数量的信息我们还用不到。我们随机选择 90% 作为训练集，10% 作为测试集，并且使用二项独立模型进行计算。然后代码实现如下：

```python
# -*- coding：utf-8 -*-
import os
import nltk
import time
import random

# 在数据集中提取出来的单词
names = ["make"，"address"，"all"，"3d"，"our"，"over"，"remove"，"internet"，"order"，"mail"，"receive"，"will"，"people"，"report"，"addresses"，"free"，"business"，"email"，"you"，"credit"，"your"，"font"，"000"，"money"，"hp"，"hpl"，"george"，"650"，"lab"，"labs"，"telnet"，"857"，"data"，"415"，"85"，"technology"，"1999"，"parts"，"pm"，"direct"，"cs"，"meeting"，"original"，"project"，"re"，"edu"，"table"，"conference"，";"，"("，"["，"!"，"$"，"#"]

dataSet = [map(float，x.split(",")) for x in open("spambase/spambase.data")]
trainData = []
for x in dataSet:
    c = x.pop()             # 最后一列为分类
    for i in range(3)：     # 紧跟的三列为单词数的一些信息，这里不需要使用
        x.pop()

    # 我们使用二项独立模型，即仅统计一个单词有没有出现
    trainData.append((dict(map(lambda (i，f)：(names[i]，f > 0)，enumerate(x)))，c))

# 随机选择10%作为测试集
testData = []
for i in range(int(len(dataSet) * 0.1)):
    idx = random.randint(0，len(trainData) - 1)
    testData.append(trainData.pop(idx))

print "train number：%d，test number：%d" % (len(trainData)，len(testData))

# 调用NLTK提供的朴素贝叶斯分类器
classifier = nltk.NaiveBayesClassifier.train(trainData)
# 打印前几个最具分类信息的特征
classifier.show_most_informative_features(10)
accuracy = nltk.classify.accuracy(classifier，testData)

print "accuracy：%.6f" % accuracy
```

执行一次，正确率大概是 87% 左右。

我们可以写一个多次执行并统计平均正确率的脚本：

```python
import os
import sys
import commands

if len(sys.argv) == 1:
    print "Please Input Python Code File Path"
    exit(0)

filename = sys.argv[1]
if not os.path.exists(filename):
    print filename，"not exists !"
    exit(0)

n = 10
if len(sys.argv) == 3:
    n = max(1，int(sys.argv[2]))

result = []
for i in range(n):
    output = commands.getoutput("python " + filename)
    print output
    result.append(float(output.split()[-1]))
    print "########### % 3d Times Finished   ###########" % (i + 1)

print "Sum:"，sum(result)
if len(result):
    print "Average:"，sum(result) / len(result)
```

保存为 runpy.py，上面的脚本保存为 text.py。在终端内执行 `python run.py text.py 20`，就会将 text.py 执行 20 次，并且将执行结果的最后的输出统计起来，最后输出总和和平均数。我就是这么懒。可以把 `alias runpy = "python /path/to/runpy.py"` 加入 bashrc 文件里，这里可以更方便的执行一些统计正确率的任务。

我执行 10 次得到的平均值是 88.2% 左右，这个正确性还是很高的。

## 使用 Python 编写朴素贝叶斯分类器

现在我们就可以写一个基于 BIM 的朴素贝叶斯分类器。当我们了解了上面讲述的所有原理后，写分类器代码还是比较简单的。数据平滑算法依然使用简单的 Laplace 平滑算法。

```python
# -*- coding：utf-8 -*-
from math import log

def train(trainSet，classifier = None):
    if classifier == None:
        classifier = {}

    for trainData，c in trainSet:
        # 如果不包含当前类别，则新建一个
        if c not in classifier:
            classifier[c] = ({}，{'size'：0})

        # 获取分类的空间向量和大小信息
        classVect，classInfo = classifier[c]
        classInfo['size'] += 1

        # 统计空间向量
        for k in trainData:
            if trainData[k]:
                classVect[k] = classVect.get(k，0) + 1
    return classifier

def classify(testData，classifier):
    # 计算总的训练集大小
    totalCount = 0
    for c in classifier:
        classVect，classInfo = classifier[c]
        totalCount += classInfo.get('size'，0)

    result = {}
    for c in classifier:
        classVect，classInfo = classifier[c]
        size = classInfo.get('size'，0)
        assert size > 0

        result[c] = log(size) - log(totalCount)

        # 二项独立模型，只考虑文档里有没有出现某个词
        for k in testData:
            if testData[k]:
                result[c] += log(classVect.get(k，0) + 1)
            else:
                result[c] += log(size - classVect.get(k，0) + 1)
            result[c] -= log(size + 2)

    result = sorted(result.items()，lambda x，y：-cmp(x[1]，y[1]))
    return result

def accuracy(testSet，classifier):
    positives = 0
    for testData，c in testSet:
        prediction = classify(testData，classifier)
        if prediction[0][0] == c:
            positives += 1
    return float(positives) / len(testSet)
```

怎么使用呢? 基本和 NLTK 里的分类器相同。将上面的文件保存为 bayesBIM.py，然后将 NLTK 那里使用分类器的代码改为：

```python
import bayesBIM as bayes

classifier = bayes.train(trainSet)
accuracy = bayes.accuracy(testSet，classifier)
print "accuracy:"，accuracy
```

大概就会输出了正确率了。

对于 MM 模型，只需要修改一下 classify 函数中计算 result[c] 的部分，以及 train 函数中的叠加部分。整个函数如下：

```python
# -*- coding：utf-8 -*-
from math import log

def train(trainSet，classifier = None):
    if classifier == None:
        classifier = {}

    for trainData，c in trainSet:
        # 如果不包含当前类别，则新建一个
        if c not in classifier:
            classifier[c] = ({}，{'size'：0})

        # 获取分类的空间向量和大小信息
        classVect，classInfo = classifier[c]
        classInfo['size'] += 1

        # 统计空间向量
        for k in trainData:
            classVect[k] = classVect.get(k，0) + float(trainData[k])
    return classifier

def classify(testData，classifier):
    # 计算总的训练集大小
    totalCount = 0
    for c in classifier:
        classVect，classInfo = classifier[c]
        totalCount += classInfo.get('size'，0)

    result = {}
    for c in classifier:
        classVect，classInfo = classifier[c]
        size = classInfo.get('size'，0)
        assert size > 0

        result[c] = log(size) - log(totalCount)

        # 多项式词频模型，考虑某个分类里出现某词的概率
        wordsCount = float(sum(classVect.values()))
        for k in testData:
            p = (classVect.get(k，0) + 1) / (wordsCount + len(classVect))
            if testData[k]:
                result[c] += float(testData[k]) * log(p)
            else:
                result[c] -= log(1 - p)
    result = sorted(result.items()，lambda x，y：-cmp(x[1]，y[1]))
    return result

def accuracy(testSet，classifier):
    positives = 0
    for testData，c in testSet:
        prediction = classify(testData，classifier)
        if prediction[0][0] == c:
            positives += 1
    return float(positives) / len(testSet)
```

使用方法相同。这里值得注意的是，train 返回的分类器 classifier 可以反复训练，假设又有一批新数据需要训练，我们可以把曾经的 classifier 当做参数传入 train 函数中。

曾经的 classifier 怎么保存呢? 使用 Python 自带的序列化保存到文件就好了嘛。代码如下：

```python
try:
    import cPickle as pickle
except ImportError:
    import pickle

# 将分类器保存进文件
def saveClassifier(filename，classifier):
    with open(filename，'wb') as f:
        pickle.dump(classifier，f)

# 从文件载入分类器
def loadClassifier(filename):
    with open(filename，'rb') as f:
        return pickle.load(f)
```

## 两种模型的测试和对比

我们在 Spambase 数据集上使用两种模型的朴素贝叶斯算法进行 1000 次测试。每次测试随机抽取 10% 作为测试集，90% 作为训练集，两种模型使用相同的数据。

对于新的测试需求，我们需要计算两个值的一些统计特征。所以重写了上面使用过的 runpy.py 文件，如下：

```python
import os
import sys
import commands
import getopt
import numpy as np

try:
    opts，args = getopt.getopt(sys.argv[1:]，"f:t:c:")
except getopt.GetoptError:
    print "Get Option Error !"
    exit(0)

filename = ''
times = 10
column = 1

for k，v in opts:
    if k == "-f":
        filename = v
    elif k == "-t":
        times = max(1，int(v))
    elif k == "-c":
        column = max(1，int(v))
    else:
        print "No Option:"，k
        exit(0)
if filename == '':
    print "Please Input Python Code File Path，like ' -f do.py'"
    exit(0)

if not os.path.exists(filename):
    print "Please Input Correct File Path,"，filename，"not exists !"
    exit(0)

results = []
for i in range(times):
    output = commands.getoutput("python " + filename)
    print output
    results.append(map(float，output.split()[-column:]))
    print "########### % 3d Times Finished   ###########" % (i + 1)

results = zip(*results)

print "_" * 78
print "|% 5s |" % "Col",
print "% 11s |" % "Sum",
print "% 11s |" % "Max",
print "% 11s |" % "Min",
print "% 11s |" % "Ave",
print "% 11s |" % "Std",
print ""

colNum = 0
for result in results:
    colNum += 1
    l = len(result)
    ave = sum(result) / l
    std = (sum(map(lambda x：(x - ave)**2，result)) / l)**0.5

    print "|% 5d |" % colNum,
    print "% 11.3f |" % sum(result),
    print "% 11.3f |" % max(result),
    print "% 11.3f |" % min(result),
    print "% 11.3f |" % ave,
    print "% 11.3f |" % std,
    print ""
```

使用类似如下：

```python
python runpy.py -f test.py -c 2 -t 1000
```

这个命令会执行 test.py 文件 1000 次，每次统计最后输出的两列数字，最后输出总和，最大值，最小值，平均值，标准差。

经过 1000 测试(午饭时间…)，我们得出的实验结果如下：

```text
______________________________________________________________________________
|  Col |         Sum |         Max |         Min |         Ave |         Std |
|  BIM |     885.696 |       0.937 |       0.835 |       0.886 |       0.014 |
|   MM |     894.667 |       0.933 |       0.846 |       0.895 |       0.013 |
```

可以看到，多项式模型会略胜一筹，正确率大概是 89.5%，比二项独立模型高出 1 个百分点左右，也稳定一点，基本差别不大。接下来我们尝试使用其他平滑算法，以及使用其他模型进行测试。

## 基于朴素贝叶斯的中文文本分类

针对中文的文本分类，我们需要加入一个步骤：分词。英文有着天然的空格分词，只要将单词的形式统一，即可统计词频信息; 而中文如果想从一句话中把每个词语都准确地解析出来则是困难的。我们这里直接使用实现好的 Python 中文分词库：[jieba](https://github.com/fxsjy/jieba)。

jieba 分词的使用很简单，详细用法可以参考它的 GitHub 主页。

对于中文的文本分类数据集，我们直接使用搜狗实验室提供的[文本分类语料库](http://www.sogou.com/labs/dl/c.html)。一般下载精简版就好了。一共 9 个类别，每个类别有 1990 个文本文件.

为了实验的效率，我们首先对每个文件进行分词，并将分词后的词频信息保存到另一个文件中。这样每个实验时我们就不需要重复分词这一步骤了，读取比分词快很多。如下：

```python
# -*- coding：utf-8 -*-
import os
import time
import random

import jieba
jieba.initialize()
# 开启4个线程，windows可能不支持
jieba.enable_parallel(4)

try:
    import cPickle as pickle
except ImportError:
    import pickle

resPath = 'sogouc.reduced'
classes = os.listdir(resPath)
for className in classes:
    classPath = resPath + '/' + className
    if not os.path.isdir(classPath):
        continue

    files = os.listdir(classPath)
    rawFiles = [x for x in files if x[-4:] == '.txt']

    for filename in rawFiles:
        startTime = time.time()

        with open(classPath + '/' + filename) as f:
            rawText = f.read()
        dicts = {}
        for word in jieba.cut(rawText):
            dicts[word] = dicts.get(word，0) + 1
        with open(classPath + '/' + filename + '.cut'，'wb') as f:
            pickle.dump(dicts，f)

        print "%s，%s，%.3f" % (className，filename，time.time() - startTime)

print "Finished"
```

我们将分词后的结果仍然存储在以前的分类目录里，加上了 .cut 后缀以作区分。

分词完成后，我们可以继续按照之前的步骤处理分类。代码如下：

```python
# -*- coding：utf-8 -*-
import os
import nltk
import time
import pylab
import random
import bayes.mm as bayes

try:
    import cPickle as pickle
except ImportError:
    import pickle

dicts = dict()
trainData = list()
testSet = list()
selectNum = 200
selectStartPos = 500
selectEndPos = 1800

resPath = 'sogouc.reduced'
classes = os.listdir(resPath)
for className in classes:
    classPath = resPath + '/' + className
    files = os.listdir(classPath)
    files = [x for x in files if x[-4:] == '.cut']
    fileNum = len(files)

    candidates = range(fileNum)
    for i in range(selectNum):
        idx = random.randint(0，len(candidates) - 1)
        fileIdx = candidates[idx]
        candidates.pop(idx)

        with open(classPath + '/' + files[fileIdx]) as f:
            wordsDict = pickle.load(f)

        for word in wordsDict:
            dicts[word] = dicts.get(word，0) + wordsDict[word]

        trainData.append((wordsDict，className))

print "dict size：%d" % len(dicts)

# 按照词频排序
sortedDict = sorted(dicts.items()，lambda x，y：cmp(y[1]，x[1]))
# 选择前一部分单词作为特征词典
features = map(lambda x：x[0]，sortedDict[selectStartPos ：selectEndPos])

# 构建训练集
# trainSet = []
# for (wordsDict，c) in trainData:
#     selectedDict = dict()
#     for x in features:
#         selectedDict[x] = wordsDict.get(x，0)
#     selectedDict['other word'] = sum(wordsDict.values()) - sum(selectedDict.values())
#     trainSet.append((selectedDict，c))

trainSet = [(dict(map(lambda w：(w，d.get(w，0))，features))，c) for (d，c) in trainData]

# 随机抽取10%作为测试集
testSet = []
for i in range(int(len(trainSet) * 0.1)):
    idx = random.randint(0，len(trainSet) - 1)
    testSet.append(trainSet.pop(idx))

print "train number：%d，test number：%d" % (len(trainSet)，len(testSet))

classifier = bayes.train(trainSet)
accuracy = bayes.accuracy(testSet，classifier)
print "accuracy：%.6f" % accuracy
```

因为文本量较大，处理时间可能略长。我们随机在每个分类中选择 200 篇文档，统计所有出现的词。而后将词频排在 500 到 1800 的词提取出来作为词特征。而后随机取 10% 作为测试集。现在我们可以继续使用 runpy 多次执行并统计正确率的平均值。因为运行的比较慢，运行的次数并不多。我执行 100 次得到的结果大概是 80.3% 左右。

### 参考文献

除了本文中提供的链接外，本文还参考了：

1. Alpaydin E。Introduction to Machine Learning[M]。MIT Press，2014.
2. Harrington P。Machine learning in action[M]。Manning Publications Co.，2012.
3. [http://blog.csdn.net/lifeitengup/article/details/12287987](http://blog.csdn.net/lifeitengup/article/details/12287987)
4. [http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html)
