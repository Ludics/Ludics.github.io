# Paper Reading on Model Compression: Compression and Acceleration

## 1. A Survey of Model Compression and Acceleration for Deep Neural Networks

### Abstract

> Deep convolutional neural networks (CNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. During the past few years, tremendous progress has been made in this area. In this paper, we survey the recent advanced techniques for compacting and accelerating CNNs model developed. These techniques are roughly categorized into four schemes: parameter pruning and sharing, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and sharing will be described at the beginning, after that the other techniques will be introduced. For each scheme, we provide insightful analysis regarding the performance, related applications, advantages, and drawbacks etc. Then we will go through a few very recent additional successful methods, for example, dynamic capacity networks and stochastic depths networks. After that, we survey the evaluation matrix, the main datasets used for evaluating the model performance and recent benchmarking efforts. Finally, we conclude this paper, discuss remaining challenges and possible directions on this topic.

### Introduction
Deep networks with billions parameters, GPUs with high computation capability. Two example: ImageNet, LFW dataset. Reducing storage & computational cost is critical, for real-time app. Solutions from many disciplines: ML, optimization, computer arch, data compression, indexing, hardware design.

Four approaches: parameter pruning & sharing, low-rank factorization, transferred/compact convolutinal filters, knowledge distillation.

### Parameter pruning & sharing 
##### Quantization & Binarization
Reducing the number of bits required to represent each weight.
1. Gong & Wu: K-means scalar quantization to the parameter values. 
2. Vanhoucke: 8-bit quantization => significant speed-up, accuracy loss minimal. 
3. 16-bit fixed-point representation in stochastic rounding based CNN training, reduced memory usage & float point operations with little loss.
4. Quantized the link weights, Huffman coding, pruning small-weight connections, achieved sota.
5. Hessian weight can measure importance of paramaters, minimize Hessian-weighted quantization errors in average.
6. Some binary weight methods, but accuracy lowered in large nets
7. Proximal Newton algorithm with diagonal Hessian approximation that directly minimizes the loss with respect to the binary weights.
8. Reduced time on float point multiplication by stochastically binarizing weights & converting multiplications to significant changes.

##### Pruning & Sharing
Reducing network complexity and to address the over-fitting issue.
1. Biased Weight Decay
2. Optimal Brain Damage & Optimal Brain Surgeon, reducing connections by Hessian of loss -> Higher Accuracy than weight decay method \(magnitude-based pruning\).
3. Srinivas & Badu, exploring redundancy among neurons, data-free method to remove redundant neurons.
4. Han, reducing total number of parameters & operations in the entire network.
5. Chen, HashedNets, low-cost hash function, group weights into hash buckets.
6. soft weight-sharing to regularize, quantization and pruning 
7. group sparsity regularization

**Drawbacks**: pruning with $L_1$ & $L_2$ regularization, more iterations to converge; manual setup of parameters

##### Designing Structural Matrix
FC layer: bottleneck of memory.  $f(\mathbf{x}, \mathbf{M}) = \sigma(\mathbf{Mx})$, while $\mathbf{x}$ is input, $\sigma(\cdot)$ is nonlinear operator, $\mathbf{M}$ is $m \times n$ matrix of paramters.

**Structured matrix**: an $m \times n$ matrix can be described using much fewer parameters than $mn$; which can reduce memory cost & accelerate inference and training stage. A simple & efficient approach is based on circulant projections. Given $\mathbf{r} =(r_0, r_1, \dots, r_{d-1})$, a circulant matrix $\mathbf{R} \in \mathbb{R}^{d \times d}$ is defined as:

$$
\mathbf{R} = \mathrm{circ}(\mathbf{r}) := 
    \begin{bmatrix} 
        r_0 & r_{d-1} & \dots & r_2 & r_1 \\ 
        r_1 & r_0 & r_{d-1} & & r_2 \\
        \vdots & r_1 & r_0 & \ddots & \vdots \\
        r_{d-2} & & \ddots & \ddots & r_{d-1} \\
        r_{d-1} & r_{d-2} & \dots & r_1 & r_0
    \end{bmatrix}
$$

The memory cost becomes $\mathcal{0}(d)$ instead of $\mathcal{0}(d^2)$. Using FFT, the computation complexity will be $\mathcal{0}(d\mathrm(log)d)$

Other methods: Adaptive Fastfood transform matrix; block and multi-level Toeplitz-like matrices; general structured efficient linear layer.

**Drawbacks**: hurt performanc; hard to find a proper structural matrix.

### Low-rank Factorization and Sparsity







