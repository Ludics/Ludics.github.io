# Paper Reading on Deep Compressed Sensing

## 1. Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling

### 3.1 Abstract

> Linear encoding of sparse vectors is widely popular, but is commonly data-independent – missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper, we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used $\mathcal{l}_1$ decoder. The convex $\mathcal{l}_1$ decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into T projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets; our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification, and empirically show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches.

### 3.2 Introduction

Design a measurement matrix $A \in \mathbb{R}^{m \times d}$, such that for an ill-posed problem $y = Ax$ where $x \in \mathbb{R}^d$ and $y \in \mathbb{R}^m$, and $m < d$. The problem of designing measurement matrices and reconstruction algorithms that recover sparse vectors from linear observations is called Compressed Sensing (CS), Sparse Approximation or Sparse Recovery Theory.

The problem can be solved as follows:

$$
\argmin _{x' \in \mathbb{R}^d} \parallel x' \parallel _0  \quad \mathrm{s.t.} \quad Ax'=y
$$

But it's a NP-Hard problem. With some properties such as Restricted Isometry Property (RIP) or the nullspace condition (NSP), $\mathcal{l}_0$ norm can be relaxed to $\mathcal{l}_1$ norm:

$$
D(A, y) = \argmin _{x' \in \mathbb{R}^d} \parallel x' \parallel _1  \quad \mathrm{s.t.} \quad Ax'=y
$$

The authors interested in vectors that are not only sparse but have additional structure in their support. They propose a data-driven algorithm that learns a good linear measurement matrix $A$ from data samples. Their linear measurements are subsequently decoded with the $\mathcal{l}_1$-minimization to estimate the unknown vector x.

Our method is an autoencoder for sparse data, with a linear encoder (the measurement matrix) and a complex non-linear decoder that approximately solves an optimization problem.

PCA is a data-driven dimensionality reduction method and an autoencoder with encoder and decoder all linear, and is provides the lowest MSE. Linear encoder has advantages, 1) easy to compute matrix-vector multiplication; 2) easy to interpret as every column of the encoding matrix can be viewed as feature embedding. Arora et al. found that pre-trained word embeddings such as GloVe and word2vec formed a good measurement matrices for text data, which need fewer mearusements than random matrces when used with $\mathcal{l}_1$-minimization.

Given $n$ sparse samples $x_1, x_2, \dots, x_n \in \mathbb{R}^d$, our problem is to find the best $A$:

$$
\min \limits_{A \in \mathbb{R}^{m \times d}} f(A), \mathrm{where} f(A) := \sum_{i=1}^{n} \parallel x_i - D(A, Ax_i) \parallel_2^2
$$

where $D(·,·)$ is the $\mathcal{l}_1$ decoder.

$f(A)$关于$A$的梯度很难求解，因为$D(\cdot, \cdot)$是由一个优化问题确定的。作者的创新点就在这里，将$\mathcal{l}_1$最小化转变成了T步的投影子梯度的更新，这样梯度就可以计算了。即
$$
\begin{aligned}
\hat{f}(A) :&= \sum_{i=1}^n \lVert x_i - \hat{x}_i\rVert^2_2, \mathrm{where} \\
\hat{x}_i &=T-\mathrm{step \, subgradient \, of\, } D(A,Ax_i), \mathrm{for}\, i = 1,2,\cdots,n.
\end{aligned}
$$
这个步骤就被称为**梯度展开**（gradient unrolling）。推导过程：

- 原始最小化问题：

$$
\min_{x'\in \mathbb{R}^d} \lVert x' \rVert_1 \quad \mathrm{s.t.} \quad Ax'=y
$$

- 映射子梯度方法的更新为

$$
x^{(t+1)} = \Pi(x^{(t)} - \alpha_t g^{(t)}), \quad \mathrm{where} \quad g^{(t)} = \mathrm{sign}(x^{(t)})
$$

其中，$\Pi$表示到凸集$\{x':Ax'=y\}$上的投影，$g^{(t)}$是符号函数，也就是$\lVert\cdot\rVert_1$在$x^{(t)}$处的子梯度，$\alpha_t$是第t个迭代的步长。
