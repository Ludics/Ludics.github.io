# Coursera: Sequence Models

### 1. Basic RNN

![](../images/ee3baa059d65f93c4b0e6468c950c761.png)

$$
\begin {aligned}
a^{<t>} &= g(W_{a} [a^{<t-1>}, x^{<t>}] + b_a) \\
\hat y^{<t>} &= g(W_{y} a^{<t>} + b_y)
\end {aligned}
$$

### 2. Language Model

* Input: $sentence$
* Output: $P(sentence)$
* Training set: large corpus of text

$$
\begin {aligned}
x^{<1>} &= \vec 0 \\
x^{<t>} &= y^{<t - 1>}, t > 1 \\
\hat y_{[j]}^{<t>} &= P(\text{word}_j | y^{<1>}, \cdots, y^{<t-1>}) \\
\mathcal L(\hat y^{<t>}, y^{<t>}) &= - \sum_i {y_i^{<t>} \log \hat y_i^{<t>}}
\end {aligned}
$$

![](../images/b155aa7ae417f4238532bce795525df3.png)

Sampling a Sequence:

![](../images/8f9e5e5bc65cf25cef420bf73f62b531.png)

### 3. Gradient

* Exploding: gradient clipping
* Vanishing: GRU, LSTM

### 4. [GRU & LSTM](https://arxiv.org/pdf/1412.3555.pdf)

Gated Recurrent Unit:
$$
\begin{aligned}
\Gamma _u &= \sigma(W_u[c^{<t-1>}, x^{<t>}] + b_u) \\
\Gamma _r &= \sigma(W_r[c^{<t-1>}, x^{<t>}] + b_r) \\
\tilde c^{<t>} &= \tanh (W_c[\Gamma _r * c^{<t - 1>}, x^{<t>}] + b_c) \\
c^{<t>} &= \Gamma _u * \tilde c^{<t>} + (1 - \Gamma _u) * c ^ {<t - 1>} \\
a^{<t>} &= c^{<t>}
\end{aligned}
$$
Long Short Term Memory:
$$
\begin{aligned}
\Gamma _u &= \sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u) \\
\Gamma _f &= \sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f) \\
\Gamma _o &= \sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o) \\
\tilde c^{<t>} &= \tanh (W_c[\Gamma _r * a^{<t - 1>}, x^{<t>}] + b_c) \\
c^{<t>} &= \Gamma _u * \tilde c^{<t>} + \Gamma _f * c ^ {<t - 1>} \\
a^{<t>} &= \Gamma _o * \tanh c^{<t>}
\end{aligned}
$$
![](../images/3003da7635dc96f71f8138402a9da192.png)

### 5. Word Embeddings 

[Learning]

### 6. Beam Search

[Learning]

### 7. Attention

[Learning]

#### Reference

1. [Sequence Models on Coursera](https://www.coursera.org/learn/nlp-sequence-models)

