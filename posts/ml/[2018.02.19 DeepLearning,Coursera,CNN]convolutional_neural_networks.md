# Coursera: Convolutional Neural Networks

### 1. Basic CNN

Concept:

* Filter
* Padding
* Stride
* $\lfloor \frac {n+2p-f} {s} + 1 \rfloor$
* Volume
* Pooling

Why CNN?

1. Parameter Sharing
2. Sparsity of Connection

### 2. Classic Networks

[LeNets-5 1998](http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf): ReLU, non-linearity activation after pooling layer

![](../images/572d2533cabaa2ff7bfa94d48110de83.png)

[AlexNet 2012](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf): Multi-GPUs, 60M parameters, easy to read

![](../images/3140f22e5f39b2bd1dc067957587c542.png)

[VGG-16 2015](https://arxiv.org/pdf/1409.1556/): 138M parameters

![](../images/eae88996f0583d94cfce7e30a8bbd4b5.png)

### 3. [ResNets](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

Residual Block:

![](../images/6cf30aa418941b69bb41af2db7aed6bf.png)

34-Layer Residual:

![](../images/67e4b6c326c36f88f0bae0596ea05177.png)

Why ResNets Work?
$$
a^{[l+2]}=ReLU(z^{[l+2]}+a^{[l]})
$$
If $w^{[l+2]}=0, b^{[l+2]}=0$, then:
$$
a^{[l+2]}=ReLU(a^{[l]}) = a^{[l]}
$$


