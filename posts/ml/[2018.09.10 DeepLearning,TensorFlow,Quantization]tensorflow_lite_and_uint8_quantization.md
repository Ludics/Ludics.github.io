# TensorFlow Lite 与定点量化

### 1. TensorFlow Lite 简介

[TensorFlow Lite](https://www.tensorflow.org/mobile/tflite/)，简称 `TF-Lite`，是 TensorFlow 针对移动端和嵌入式设备提出的解决方案。在尽量保持精度的情况下，极大地优化模型体积，在移动设备上更快地完成 Inference。

TF-Lite 同时支持浮点数模型和定点量化模型，其中浮点数模型可以直接通过[官方提供的 Converter 转换得到](https://www.tensorflow.org/mobile/tflite/devguide)，而量化模型则一般需要进行伪量化 + `fine-tuning` 操作。定点量化模型可以获得更快的推导速度、更小的模型体积，当然精度也下降地更厉害。

TensorFlow 官方提供了一系列 pre-trained 的模型，包括 [`MobileNet`](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md) 系列和 [`Inception`](https://github.com/tensorflow/models/tree/master/research/slim/nets) 系列等，并且也提供了[移动端上模型的参考推导时间](https://www.tensorflow.org/mobile/tflite/performance)。从数据上来看，MobileNetV1 这样的轻量级模型经过量化后，在 iPhone8 上完成一次推导仅需 24.4ms，帧率可以做到 40+ 帧/秒，可以应用到实时视频的 App 里了。

### 2. 量化简介

TF-Lite 定点量化的原理可以参考 Google 发表在 CVPR 2018 的论文，[见参考文献 1](http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf)。笔者正在阅读中，待阅读完成后补充。

[未完待续]

### 3. 量化 Example

对于量化，TensorFlow 也有[官方的样例](https://www.tensorflow.org/performance/quantization)，但对新手并不友好。为此笔者在 GitHub 上建立了一个非常简单的样例，并且配置了 CI，可以最简化地体验创建一个量化模型的完整步骤。Repo 地址：[SF-Zhou/tensorflow-quantization-example](https://github.com/SF-Zhou/tensorflow-quantization-example)

核心步骤包括：

1. 创建 FakeQuant Training Graph ([train.py](https://github.com/SF-Zhou/tensorflow-quantization-example/blob/master/train.py))
2. 导出 FakeQuant Inference Graph ([test.py](https://github.com/SF-Zhou/tensorflow-quantization-example/blob/master/test.py))
3. Freeze Graph & Checkpoint ([freeze.sh](https://github.com/SF-Zhou/tensorflow-quantization-example/blob/master/freeze.sh))
4. 导出 TF-Lite 模型 ([quantization.sh](https://github.com/SF-Zhou/tensorflow-quantization-example/blob/master/quantization.sh))

最后会生成后缀为 `.tflite` 的模型文件。该模型文件可以使用 [Netron](https://github.com/lutzroeder/Netron) 工具查看，如下图所示。

![](../images/01958c882584b62fc3b83346f49cdd98.png)

TF-Lite 模型可以在 PC 做 Inference 测试，在该 Repo 中也提供了一个简单的示例：[load_tflite.py](https://github.com/SF-Zhou/tensorflow-quantization-example/blob/master/load_tflite.py)。

### 4. TF-Lite 模型存储结构

如果需要对 TF-Lite 模型进行手术般的修改的话，那就一定要了解 TF-Lite 模型的存储结构。TF-Lite 使用了 Google 提出的 [FlatBuffers](https://github.com/google/flatbuffers) 序列化协议，简而言之就是二进制版的 JSON。TF-Lite 模型定义文件可以在 TensorFlow 项目中找到：[schema.fbs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/schema/schema.fbs)，通过 schema 文件和 FlatBuffers，可以编译得到解析 TF-Lite 模型文件的头文件，进而对模型进行进一步操作。

同样地，笔者在 GitHub 上建立了一个非常简单的样例，并且配置了 CI，以供参考。Repo 地址：[SF-Zhou/tflite-model-parse](https://github.com/SF-Zhou/tflite-model-parse)。

事实上，如果已经有其他平台的定点量化模型文件，其实可以直接通过复制 weight + bias 和量化数据的方式，制作相同精度的 TF-Lite 模型。笔者近期在工作中就有类似的尝试，验证的结果也确实是可行的。当然目前遇到了以下两个问题：

1. TF-Lite 模型中的 bias 强制为 int32 类型而非浮点数，这一点可能和其他平台的量化模型有差异导致精度上不一致；
2. TF-Lite 模型会强制 Pooling 结构的输入输出 scale 保持一致，对于 Avg Pooling 这种结构，输入的 max/min 范围一般会远大于输出的 max/min 范围，导致输出的精度降低。不过这一点影响有限。

### 参考文献

1. [Jacob, Benoit, et al. "Quantization and training of neural networks for efficient integer-arithmetic-only inference." *arXiv preprint arXiv:1712.05877* (2017).](http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf)

