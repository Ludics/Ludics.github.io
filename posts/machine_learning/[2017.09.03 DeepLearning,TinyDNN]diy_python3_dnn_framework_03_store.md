# 从零开始构建深度学习框架（三）

## 五、存储

很久没更新了。当一个模型训练好之后，需要将其存储下来，以便之后使用。这就会牵涉到两个操作：save & load。模型，不仅包括了每一层的参数，同时也包括了层与层之间的结构。也因此，TinyDNN 的源代码需要做相应的重构。

模型 save 和 load 的数据类型，选用比较常用的 JSON 格式。例如：

```json
[
  {
    "name": "input",
    "type": "InputLayer"
  },
  {
    "name": "fc1",
    "type": "FullConnect",
    "activation": "relu",
    "neuron_number": 20,
    "parameters": {
      "w": [array],
      "b": [array]
    }
  },
  {
    "name": "fc2",
    "type": "FullConnect",
    "activation": "relu",
    "neuron_number": 4,
    "parameters": {
      "w": [array],
      "b": [array]
    }
  },
  {
    "name": "loss",
    "type": "SoftmaxLayer",
    "class_number": 2,
    "parameters": {
      "w": [array],
      "b": [array]
    }
  }
]
```

该网络结构包含两个隐藏层，皆为使用 relu 激活的全连接层。最后使用 Softmax 实现分类并计算 loss。

而目前需要实现的便是：

1. 从 JSON 中读取网络结构并构建模型；
2. 将当前的模型导出为 JSON 数据。

突然发现，如果需要反复修改代码并执行测试，Jupyter 会是一个不错的工具，所以在腾讯云的服务器上搭了一个 JupyterHub 的环境。本期的操作将在 Jupyter 上完成。

首先，处理依赖：

```python
import json
import typing
import numpy as np
```

然后，将之前实现的激活层复制过来，实现一个工厂方法用于创建激活层。

使用 JSON 解析网络结构。输入层部分，目前并不确定输入层的具体实现，先省略这部分。而 Layer 的参数中，`name` 和 `type` 是确定的，其他与 Layer 本身相关的，均放置于 `parameters` 中。

```python
define_net = """
[
  {
    "name": "fc1",
    "type": "FullConnectLayer",
    "parameters": {
        "activation": "relu",
        "neuron_number": 20
    }
  },
  {
    "name": "fc2",
    "type": "FullConnectLayer",
    "parameters": {
        "activation": "relu",
        "neuron_number": 4
    }
  },
  {
    "name": "loss",
    "type": "SoftmaxLayer",
    "parameters": {
        "class_number": 2
    }
  }
]
"""
net_description = json.loads(define_net)
```

下面继续实现 Layer。首先是基类的定义。`type` 可以直接从类名获得，故而直接定义为 `property`。`description` 则返回 Layer 的完整参数。

```python
class Layer:
    def __init__(self):
        self.name = ''

    @property
    def type(self) -> str:
        return self.__class__.__name__

    @property
    def description(self) -> dict:
        return {
            'name': self.name,
            'type': self.type,
            'parameters': self.parameters
        }

    @property
    def parameters(self) -> dict:
        return {}

    def init_parameters(self, parameters: dict):
        pass
```

定义 FullConnectLayer。实现 `init_parameters` 方法和 `parameter` 属性的处理即可。

```python
class FullConnectLayer(Layer):
    def __init__(self):
        super().__init__()

        self.activation = Activation()
        self.neuron_number = 0
        self.w = np.array([])
        self.b = np.array([])

    @property
    def parameters(self) -> dict:
        return {
            'activation': self.activation.name(),
            'neuron_number': self.neuron_number,
            'w': self.w.tolist(),
            'b': self.b.tolist()
        }

    def init_parameters(self, parameters: dict):
        self.activation = ActivationFactory(parameters['activation'])
        self.neuron_number = parameters['neuron_number']

        if 'w' in parameters and 'b' in parameters:
            self.w = np.array(parameters['w'])
            self.b = np.array(parameters['b'])
```

同时也实现一个简单的 Layer 工厂方法：

```python
def LayerFactory(description: dict):
    type_name = description['type']

    layer_class = eval(type_name)
    layer = layer_class()
    layer.name = description['name']
    layer.init_parameters(description['parameters'])

    return layer
```

最后实现 Net，很简单，主要是调用 Layer 已经实现好的方法。

```python
class Net:
    def __init__(self):
        self.indexes = {}
        self.layers = []

    @property
    def description(self):
        return [layer.description for layer in self.layers]

    def from_description(self, net_description: typing.List[dict]):
        for idx, description in enumerate(net_description):
            name = description['name']
            self.indexes[name] = idx
            self.layers.append(LayerFactory(description))

    def get_layer(self, name: str) -> Layer:
        idx = self.indexes[name]
        return self.layers[idx]

    def get_full_connect_layer(self, name: str) -> FullConnectLayer:
        return self.get_layer(name)

    def get_softmax_layer(self, name: str) -> SoftmaxLayer:
        return self.get_layer(name)
```

最后做一个简单的测试：

```python
net = Net()
net.from_description(net_description)

net_description = net.description
print(json.dumps(net_description, indent=2))
```

输出为：

```json
[
  {
    "name": "fc1",
    "parameters": {
      "w": [],
      "b": [],
      "activation": "Relu",
      "neuron_number": 20
    },
    "type": "FullConnectLayer"
  },
  {
    "name": "fc2",
    "parameters": {
      "w": [],
      "b": [],
      "activation": "Relu",
      "neuron_number": 4
    },
    "type": "FullConnectLayer"
  },
  {
    "name": "loss",
    "parameters": {
      "class_number": 2
    },
    "type": "SoftmaxLayer"
  }
]
```

文中所述的 NoteBook 存在 GitHub 上，之后会和源代码本身进行整合。

[链接：day-03](https://github.com/SF-Zhou/TinyDNN/blob/day-03/notebooks/01_load_and_store.ipynb)